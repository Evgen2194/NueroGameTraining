Техническое Задание: Интерактивная Обучающая Платформа для Игрового ИИ ("Проект Агент")
1. Общая концепция
Создать десктопное приложение на Python, которое позволяет пользователю в интерактивном режиме обучать нейросетевого агента играть в 2D-игры. Агент должен обучаться как базовым механикам (навигация, клики, перетаскивания), так и долгосрочной стратегии, используя гибридную систему наград.
2. Ключевые технологии и параметры
Входные данные ("Зрение"): Цветные (RGB) изображения, сжатые до разрешения 128x128 пикселей.
Память о движении: Используется метод стекинга 4-х последних кадров (Frame Stacking) для понимания анимаций и динамики. Входной тензор для нейросети будет иметь размерность [batch_size, 12, 128, 128] (4 кадра * 3 RGB канала = 12 каналов).
Нейросеть: Единая сверточная нейронная сеть (CNN) с двумя "головами":
Action Choice Head: Классифицирует тип действия (клик, перетаскивание, ждать).
Action Parameters Head: Предсказывает параметры действия (координаты).
Обучение: Интерактивное, с запуском сессии обучения после каждых 50 оценок пользователя. Обучение происходит на основе как положительных, так и отрицательных примеров.
3. Пользовательский интерфейс и взаимодействие (Режим "Коучинга")
Запуск и настройка:
При запуске программы пользователь должен иметь возможность выделить на экране прямоугольную область, соответствующую окну игры.
Цикл "Предложение-Подтверждение":
Агент не выполняет действия сам. Он только генерирует "предложение" о следующем действии.
Пауза и Визуализация: В момент генерации предложения игровой процесс должен быть визуально "заморожен". Поверх окна игры должен отображаться полупрозрачный скриншот того состояния (стека из 4-х кадров), на основе которого агент принял решение. Это ключевое требование, чтобы пользователь точно понимал, на что реагирует нейросеть.
На этом "замороженном" скриншоте должна быть нарисована визуализация предложенного действия:
Клик: Яркий полупрозрачный круг в целевой точке.
Перетаскивание: Яркая стрелка от начальной до конечной точки.
Ожидание: Крупная надпись "ПРЕДЛОЖЕНИЕ: ЖДАТЬ" по центру экрана.
Программа останавливается и ожидает ввода от пользователя.
Система управления "Коуча" (горячие клавиши):
Стрелка вверх: ОДОБРИТЬ. Действие считается правильным (immediate_reward = +1). Оно немедленно выполняется. Агент переходит к анализу следующего состояния.
Стрелка вниз: ОТКЛОНИТЬ. Действие считается неправильным (immediate_reward = -1). Оно не выполняется. Агент немедленно генерирует новое предложение для этого же "замороженного" состояния.
Стрелка влево: ПРИНУДИТЬ К ОЖИДАНИЮ. Пользователь сообщает, что в данной ситуации правильное действие — ждать. Записывается опыт с immediate_reward = +1 для действия "ждать". Агент ждет 1-2 секунды и анализирует новое состояние.
Пробел: ПРОПУСТИТЬ/ДРУГОЕ. Текущее предложение игнорируется, награда 0. Агент генерирует новое предложение для этого же состояния.
4. Стратегическое обучение (Режим "Миссии") - Опциональный контур
Этот режим активируется и управляется специальными командами пользователя, позволяя обучать агента долгосрочному планированию.
Управление миссией:
Клавиша "1": НАЧАТЬ МИССИЮ. Пользователь нажимает "1" вместо стрелки вверх, когда агент предлагает нажать кнопку "Старт" или "Начать уровень". Это сигнализирует программе о начале сбора стратегического опыта.
Клавиша "2": МИССИЯ ВЫИГРАНА. Пользователь нажимает в конце игрового цикла (например, на экране итоговой победы).
Клавиша "3": МИССИЯ ПРОИГРАНА. Пользователь нажимает на экране итогового поражения.
Логика сбора стратегических данных:
Когда миссия активна, все одобренные пользователем действия (клик, перетаскивание), кроме ожидания, сохраняются во временный буфер миссии.
Когда пользователь нажимает "2" или "3", миссия завершается.
Все действия из буфера получают дополнительную стратегическую награду (strategic_reward = +1 за победу, strategic_reward = -1 за поражение).
Эти обновленные записи опыта добавляются в основную память.
Обучение стратегии:
После завершения миссии и присвоения стратегических наград немедленно запускается сессия обучения.
Алгоритм обучения должен учитывать сумму немедленной и стратегической наград при расчете ценности действий, таким образом отдавая предпочтение тем локально правильным действиям, которые к тому же привели к глобальной победе.
5. Сохранение и загрузка
Программа должна автоматически сохранять весь накопленный опыт (память) и состояние обученной модели (веса нейросети) в файл при выходе.
При запуске программа должна автоматически загружать эти данные, позволяя продолжить обучение с того места, где оно было прервано.